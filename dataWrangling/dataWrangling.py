import ast
import time
from functools import reduce
from common import read_gbq, to_gbq
from google.cloud import bigquery
from dataSources.deleteRowsAppleGoogle import rawDataset, googleScraped_table_name
from pyspark.sql.functions import col, regexp_replace, split, expr, udf, when, lit
from pyspark.sql.types import ArrayType, StructType, StructField, FloatType, StringType
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.sql.functions import col, udf
from pyspark.ml import Pipeline
from langdetect import detect

# Hard-coded variables
cleanDataset = "prod_cleanData" # Schema/Dataset
cleanGoogleMainScraped_table_name = 'cleanGoogleMain' # Table
cleanGoogleReviewScraped_table_name = 'cleanGoogleReview' # Table
cleanAppleMainScraped_table_name = 'cleanAppleMain' # Table
cleanAppleReviewScraped_table_name = 'cleanAppleReview' # Table

googleMain = "googleMain" # Table
googleReview = "googleReview" # Table
appleMain = "appleMain" # Table
appleReview = "appleReview" # Table

# Language Detection Packages
# 0. Generic
    # import nltk
    # from nltk.corpus import words
    # from nltk.tokenize import word_tokenize
    # nltk.download('words')
    # nltk.download('punkt')

# # 1. Polyglot
# from polyglot.detect import Detector

# 2. Langdetect (chosen method!)
# from langdetect import detect

# # 3. Gcld3 (Google Compact Language Detector 3)
# import gcld3

def appleDataWrangling(spark, project_id, client, local = False, sparkDf = None, sparkRvDf = None):

    # Apple functions
    def remove_string_from_column_apple(column, string):
        return regexp_replace(col(column), string, "")

    def helper_apple(df, string, column):
        return df.withColumn(column, remove_string_from_column_apple(column, string))

    def remove_strings_from_df_apple(df, column, strings):
        return reduce(lambda acc, string: helper_apple(acc, string, column), strings, df)

    def remove_strings_from_columns_apple(df, strings_to_remove):
        return reduce(lambda acc, column: remove_strings_from_df_apple(acc, column, strings_to_remove[column]), strings_to_remove, df)

    def remove_strings_apple(content, strings_to_remove):
        return reduce(lambda acc, s: acc.replace(s, ""), strings_to_remove, content)

    def detect_language_langdetect_apple(text):
        try:
            return detect(text)
        except:
            return "unknown"

    # Apple Data Wrangling
    def clean_data_appleMain(df):
        # Drop specific columns
        columns_to_drop = ['operatingSystem', 'authorurl']
        df = df.drop(*columns_to_drop)

        # Remove specified strings from specified columns
        strings_to_remove = {
        'description': ['<b>']
        }
        
        df = remove_strings_from_columns_apple(df, strings_to_remove)
        
        # Transform star_ratings column
        # Remove characters not needed [, ], (, ), ', %
        df = df.withColumn('cleaned_star_ratings', regexp_replace('star_ratings', r"[(|)|'|\]|\[|%]", ""))
        
        splitCol = split(df['cleaned_star_ratings'], ',')
        
        df = df.withColumn('5starrating_no', splitCol.getItem(1).cast('int')) \
            .withColumn('4starrating_no', splitCol.getItem(3).cast('int')) \
                .withColumn('3starrating_no', splitCol.getItem(5).cast('int')) \
                    .withColumn('2starrating_no', splitCol.getItem(7).cast('int')) \
                        .withColumn('1starrating_no', splitCol.getItem(9).cast('int'))
                        
        df = df.drop('star_ratings').drop('cleaned_star_ratings')
        
        # Remove &amp; in applicationCategory
        df = df.withColumn('applicationCategory', regexp_replace('applicationCategory', r"&amp;", "&"))
        
        # Change column names to align with googleMain
        df = df.selectExpr('name as title', 
                            'description', 
                            'applicationCategory as genre', 
                            'datePublished as released', 
                            'authorname as developer', 
                            'ratingValue as score', 
                            'reviewCount as reviews', 
                            'price', 
                            'priceCurrency as currency', 
                            'appId', 
                            '1starrating_no', 
                            '2starrating_no', 
                            '3starrating_no', 
                            '4starrating_no', 
                            '5starrating_no')
        
        # Drop records where 'ratingValue' column has a value of nan
        df = df.filter((col("ratingValue") != 'nan'))
        
        return df

    def clean_data_appleReview(spark, df, ref_appid_sparkDf):
        # Drop specific columns
        # No columns to drop
        
        # Filter rows where there is a mismatch in data with header columns
        df = df.filter((col("developerResponseId") == 'nan') & (col("developerResponseBody") == 'nan') & (col("developerResponseModified") == 'nan'))
        
        # Filter only records where app_id in df is in ref_appid_sparkDf.select("app_Id").distinct()
        df = df.join(ref_appid_sparkDf.select("appId").distinct(), "appId", "inner")
        
        # Drop duplicates
        df = df.dropDuplicates(['id'])
        
        # Rename and rearrnge columns to align with googleReview and drop type, offset, nBatch columns
        df = df.selectExpr('appId', 
                        'id as reviewId', 
                        'userName', 
                        'review as content', 
                        'rating as score', 
                        'date as at', 
                        'developerResponseBody as replyContent',
                        'isEdited', 'title', 'developerResponseId', 'developerResponseModified')

        # Remove specific strings from specific columns
        # Running into an error - commenting out first.
        strings_to_remove = ["<b>"]
        
        remove_strings_udf = udf(lambda x: remove_strings_apple(x, strings_to_remove), StringType())
        
        df = df.withColumn("content", remove_strings_udf(col("content")))
        
        # 2. Langdetect & Text Analytics
        # Define a UDF to apply language detection
        detect_language_udf = spark.udf.register("detect_language_udf", detect_language_langdetect_apple)
        
        # Apply language detection to the 'content' column
        df = df.withColumn("language_langdetect", detect_language_udf("content"))

        # Filter only language_langdetect = 'en'
        df = df.filter(df['language_langdetect'] == 'en')

        #TA
        custom_stopwords = ["don", "should", "now", "need", "working", "without", "doge", "screen", "app.",
        "first", "cant", "completely", "won't", "make", "still", "definitions",  "i'm", "many",
        "want", "game", "don't", "even", "can't", "doesn't", "worst", "it's",
        "one", "open", "work", "get", "people", "like", "good",  "nothing", "every", "would", "words",
        "actually", "the", "and", "to", "i", "app", "a", "of", "not", "it", "this", "is", "in", "your", "you", "very", "but", "for", "are", "they", "time", "have", "no", "please", "with", "so", "that", "bad",
        "app","will","ok","u","please","great","i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your" "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself",
        "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these",
        "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do",
        "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while",
        "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before",
        "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again",
        "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each",
        "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than",
        "too", "very", "s", "t", "can", "will", "just", "don", "should", "now","thank","sorry","could","you."]

        # Create TA Pipeline
        tokenizer = Tokenizer(inputCol="content", outputCol="tokens")
        stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol="filtered_tokens", stopWords=custom_stopwords)
    
        pipeline = Pipeline(stages=[tokenizer, stopwords_remover])
        TA_pipeline = pipeline.fit(df)

        #transformed_df holds the result of applying this pipeline to original df, performing tokenization, stop word removal
        df = TA_pipeline.transform(df)

        return df

    #################################################### appleMain
    if not local: 
        cleanAppleScraped_db_path = f"{project_id}.{cleanDataset}.{cleanAppleMainScraped_table_name}" # Schema + Table
        sparkDf = read_gbq(spark, rawDataset, appleMain)  # reinstate at production
        
        # print(sparkDf.count())
    
    # Code section for cleaning appleMain data
    cleaned_sparkDf = clean_data_appleMain(sparkDf)
    
    if not local: 
        client.create_table(bigquery.Table(cleanAppleScraped_db_path), exists_ok = True)
        to_gbq(cleaned_sparkDf, cleanDataset, cleanAppleMainScraped_table_name)  # reinstate at production
    
    #################################################### appleReview
    if not local: 
        cleanAppleScraped_db_path = f"{project_id}.{cleanDataset}.{cleanAppleReviewScraped_table_name}" # Schema + Table
        sparkDf = read_gbq(spark, rawDataset, appleReview)  # reinstate at production
        
    else: 
        sparkDf = sparkRvDf
    
    # print(sparkDf.count())

    time.sleep(30)
    
    if not local: 
        ref_appid_sparkDf = read_gbq(spark, cleanDataset, cleanAppleMainScraped_table_name)  # reinstate at production
        pass
    else: 
        ref_appid_sparkDf = cleaned_sparkDf
        
    # print(sparkDf.show())
    # print(ref_appid_sparkDf.count())

    # Code section for cleaning appleReview data
    cleaned_sparkDf = clean_data_appleReview(spark, sparkDf, ref_appid_sparkDf)

    client.create_table(bigquery.Table(cleanAppleScraped_db_path), exists_ok = True)  # reinstate at production

    # print('Table created successfully')

    to_gbq(cleaned_sparkDf, cleanDataset, cleanAppleReviewScraped_table_name)  # reinstate at production

    # print('Table sent to GBQ successfully')


# Google Data Wrangling
def googleDataWrangling(spark, project_id, client):

    # googleMain
    
    cleanGoogleScraped_db_path = f"{project_id}.{cleanDataset}.{cleanGoogleMainScraped_table_name}" # Schema + Table

    sparkDf = read_gbq(spark, rawDataset, googleMain)
    # print(sparkDf.show())
    # print(sparkDf.count())

    # Code section for cleaning googleMain data
    def clean_data_googleMain(df):

        # Drop specific columns
        columns_to_drop = ['descriptionHTML', 'sale', 'saleTime', 'originalPrice', 'saleText', 'developerId', 'developerAddress', 'containsAds', 'updated']
        df = df.drop(*columns_to_drop)

        # Remove specified strings from specified columns
        strings_to_remove = {
        'description': ['<b>']
        }
        # for column, strings in strings_to_remove.items():
        #     for string in strings:
        #         df = df.withColumn(column, regexp_replace(col(column), string, ""))

        def remove_strings_from_columns(df, strings_to_remove):
            def remove_string_from_column(column, string):
                return regexp_replace(col(column), string, "")

            # def remove_strings_from_df(df, column, strings):
            #     for string in strings:
            #         df = df.withColumn(column, remove_string_from_column(column, string))
            #     return df
            
            def remove_strings_from_df(df, column, strings):
                def helper(df, string):
                    return df.withColumn(column, remove_string_from_column(column, string))
                
                return reduce(lambda acc, string: helper(acc, string), strings, df)


            return reduce(lambda acc, column: remove_strings_from_df(acc, column, strings_to_remove[column]), strings_to_remove, df)

        df = remove_strings_from_columns(df, strings_to_remove)

        # Convert categories list dictionary into list
        # Define the extract_names_udf function as a UDF (Purely Functional Programming)

        def extract_names_udf(data):
            elements = ast.literal_eval(data)
            return list(map(lambda item: item['name'], elements))

                        # def extract_names_udf(data):
                        #     elements = ast.literal_eval(data)
                        #     names = [item['name'] for item in elements] 
                        #     return names
        
        extract_names = udf(extract_names_udf, ArrayType(StringType()))
        
        # Apply the UDF to the "categories" column
        df = df.withColumn("categories_list", extract_names("categories"))
        # Convert the categories_list column to string datatype
        df = df.withColumn("categories_list", col("categories_list").cast(StringType()))
        df = df.drop("categories")
    
        
        # Drop records where 'ratings' column has a value of 0, nan or None
        df = df.filter((col("ratings") != 0) & (col("ratings") != 'None'))
                    #    & (~col("ratings") != 'nan') & (col("ratings") != 'None'))

        # Drop records where 'minInstalls' column is None
        df = df.filter((col("minInstalls") != 0) & (col("minInstalls") != 'None'))
                    #    & (~col("minInstalls") != 'nan') & (col("minInstalls") != 'None'))
        
        # # Splitting the column based on ' - ' and ' per item' and selecting the appropriate elements
        # df = df.withColumn('price_range', split(col('inAppProductPrice'), ' - '))
        # df = df.withColumn('min_inAppProductPrice', when(col('price_range').getItem(0).contains('$'), col('price_range').getItem(0).substr(2, 4)).otherwise(None))
        # df = df.withColumn('max_inAppProductPrice', when(col('price_range').getItem(1).contains('$'), col('price_range').getItem(1).substr(2, 4)).otherwise(None))

        # Splitting the column based on ' - ' and ' per item' and selecting the appropriate elements
        df = df.withColumn('price_range', split(col('inAppProductPrice'), ' - '))
        df = df.withColumn('min_inAppProductPrice', when(col('price_range').getItem(0).contains('$'), regexp_replace(col('price_range').getItem(0), '\\$', '').substr(1, 5)).otherwise(None))
        df = df.withColumn('max_inAppProductPrice', when(col('price_range').getItem(1).contains('$'), regexp_replace(col('price_range').getItem(1), '\\$', '').substr(1, 6)).otherwise(None))

        # Dropping the intermediate column and displaying the DataFrame
        df = df.drop('price_range')

        # Split 'histogram' column into 5 columns
        df = df.withColumn("histogram", expr("substring(histogram, 2, length(histogram) - 2)")) # Remove brackets []
        df = df.withColumn("histogram", split(col("histogram"), ", ")) # Split into array
        df = df.withColumn("1starrating_no", df.histogram.getItem(0).cast("int")) \
            .withColumn("2starrating_no", df.histogram.getItem(1).cast("int")) \
            .withColumn("3starrating_no", df.histogram.getItem(2).cast("int")) \
            .withColumn("4starrating_no", df.histogram.getItem(3).cast("int")) \
            .withColumn("5starrating_no", df.histogram.getItem(4).cast("int")) \
            .drop("histogram")

        return df
    
    cleaned_sparkDf = clean_data_googleMain(sparkDf)

    client.create_table(bigquery.Table(cleanGoogleScraped_db_path), exists_ok = True)
    to_gbq(cleaned_sparkDf, cleanDataset, cleanGoogleMainScraped_table_name)

    #################################################### googleReview
    
    cleanGoogleScraped_db_path = f"{project_id}.{cleanDataset}.{cleanGoogleReviewScraped_table_name}" # Schema + Table

    sparkDf = read_gbq(spark, rawDataset, googleReview)
    # print(sparkDf.show())
    # print(sparkDf.count())

    time.sleep(30)
    ref_appid_sparkDf = read_gbq(spark, cleanDataset, cleanGoogleMainScraped_table_name)
    # print(sparkDf.show())
    # print(ref_appid_sparkDf.count())

    # Code section for cleaning googleReview data
    def clean_data_googleReview(df):

        # Drop specific columns
        columns_to_drop = ['appVersion']
        df = df.drop(*columns_to_drop)

       # Filter only records where app_id in df is in ref_appid_sparkDf.select("app_Id").distinct()
        df = df.join(ref_appid_sparkDf.select("appId").distinct(), "appId", "inner")

        # Drop duplicates
        df = df.dropDuplicates(['reviewId'])

        # Remove specific strings from specific columns

        strings_to_remove = ["<b>"]
     
        def remove_strings(content, strings_to_remove):
            return reduce(lambda acc, s: acc.replace(s, ""), strings_to_remove, content)

        remove_strings_udf = udf(lambda x: remove_strings(x, strings_to_remove), StringType())

        df = df.withColumn("content", remove_strings_udf(col("content")))

        ### Language Detection Section

        # Use across all Spark session:               detect_language_udf = spark.udf.register("detect_language_udf", detect_language_function)
        # Use on current Dataframe and spark session: detect_language_udf = udf(detect_language_function, StringType())

        # # 1. Polyglot
        # def detect_language_polyglot(text):
        #     try:
        #         detector = Detector(text)
        #         return detector.language.code
        #     except:
        #         return "unknown"

        # # Define a UDF to apply language detection
        # detect_language_udf = spark.udf.register("detect_language_udf", detect_language_polyglot)

        # # Apply language detection to the 'content' column
        # df = df.withColumn("language_ployglot", detect_language_udf("content"))

        # 2. Langdetect & Text Analytics
        def detect_language_langdetect(text):
            try:
                return detect(text)
            except:
                return "unknown"
            
        # Define a UDF to apply language detection
        detect_language_udf = spark.udf.register("detect_language_udf", detect_language_langdetect)

        # Apply language detection to the 'content' column
        df = df.withColumn("language_langdetect", detect_language_udf("content"))


        # # 3. gcld3
        # def detect_language_gcld3(text):
        #     try:
        #         detector = gcld3.NNetLanguageIdentifier(min_num_bytes=0, max_num_bytes=1000)
        #         result = detector.FindLanguage(text=text)
        #         lang_detected = result.language
        #         return lang_detected
        #     except:
        #         return "unknown"

        # # Define a UDF to apply language detection
        # detect_language_udf = spark.udf.register("detect_language_udf", detect_language_gcld3)

        # # Apply language detection to the 'content' column
        # df = df.withColumn("language_gcld3", detect_language_udf("content"))

        # Filter only language_langdetect = 'en'
        df = df.filter(df['language_langdetect'] == 'en')

    ########################### Wudi to insert text#################################
    ########################### TA - Start [Wudi]#################################
        #Remove stopwords
        custom_stopwords = ["don", "should", "now", "need", "working", "without", "doge", "screen", "app.",
        "first", "cant", "completely", "won't", "make", "still", "definitions",  "i'm", "many",
        "want", "game", "don't", "even", "can't", "doesn't", "worst", "it's",
        "one", "open", "work", "get", "people", "like", "good",  "nothing", "every", "would", "words",
        "actually", "the", "and", "to", "i", "app", "a", "of", "not", "it", "this", "is", "in", "your", "you", "very", "but", "for", "are", "they", "time", "have", "no", "please", "with", "so", "that", "bad",
        "app","will","ok","u","please","great","i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your" "he", "him", "his", "himself", "she", "her", "hers", "herself", "it", "its", "itself",
        "they", "them", "their", "theirs", "themselves", "what", "which", "who", "whom", "this", "that", "these",
        "those", "am", "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "having", "do",
        "does", "did", "doing", "a", "an", "the", "and", "but", "if", "or", "because", "as", "until", "while",
        "of", "at", "by", "for", "with", "about", "against", "between", "into", "through", "during", "before",
        "after", "above", "below", "to", "from", "up", "down", "in", "out", "on", "off", "over", "under", "again",
        "further", "then", "once", "here", "there", "when", "where", "why", "how", "all", "any", "both", "each",
        "few", "more", "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "so", "than",
        "too", "very", "s", "t", "can", "will", "just", "don", "should", "now"]

        # Create TA Pipeline
        tokenizer = Tokenizer(inputCol="content", outputCol="tokens")
        stopwords_remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol="filtered_tokens", stopWords=custom_stopwords)
       #hashingTF = HashingTF(inputCol=stopwords_remover.getOutputCol())
       #idf= IDF(inputCol=hashingTF.getOutputCol())

        pipeline = Pipeline(stages=[tokenizer, stopwords_remover])
        TA_pipeline = pipeline.fit(df)

        #transformed_df holds the result of applying this pipeline to original df, performing tokenization, stop word removal
        df = TA_pipeline.transform(df)
        
    ########################### TA - End #################################
        return df

    cleaned_sparkDf = clean_data_googleReview(sparkDf)

    client.create_table(bigquery.Table(cleanGoogleScraped_db_path), exists_ok = True)

    # print('Table created successfully')

    to_gbq(cleaned_sparkDf, cleanDataset, cleanGoogleReviewScraped_table_name)

    # print('Table sent to GBQ successfully')
